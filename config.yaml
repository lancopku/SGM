data: './data/data/save_data'
log: './data/data/log/'
epoch: 50
batch_size: 64
param_init: 0.1
optim: 'adam'
learning_rate: 0.001
max_grad_norm: 10
learning_rate_decay: 0.5
global_emb: False
mask: True
schedule: True
bidirec: True
start_decay_at: 5
emb_size: 256
encoder_hidden_size: 256
decoder_hidden_size: 512
num_layers: 2
dropout: 0.5
max_tgt_len: 25
eval_interval: 100
save_interval: 3000
max_generator_batches: 32
metric: ['hamming_loss', 'micro_f1']
shared_vocab: False
beam_size: 5